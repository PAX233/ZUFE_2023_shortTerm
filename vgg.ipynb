{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG\n",
    "计算机视觉是一直深度学习的主战场，从这里我们将接触到近几年非常流行的卷积网络结构，网络结构由浅变深，参数越来越多，网络有着更多的跨层链接，首先我们先介绍一个数据集 cifar10，我们将以此数据集为例介绍各种卷积网络的结构。\n",
    "\n",
    "## CIFAR 10\n",
    "cifar 10 这个数据集一共有 50000 张训练集，10000 张测试集，两个数据集里面的图片都是 png 彩色图片，图片大小是 32 x 32 x 3，一共是 10 分类问题，分别为飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车。这个数据集是对网络性能测试一个非常重要的指标，可以说如果一个网络在这个数据集上超过另外一个网络，那么这个网络性能上一定要比另外一个网络好，目前这个数据集最好的结果是 95% 左右的测试集准确率。\n",
    "\n",
    "![image.png](attachment:9bc17506-cd65-45d3-9307-88ed3629b8bb.png)\n",
    "\n",
    "你能用肉眼对这些图片进行分类吗？\n",
    "\n",
    "cifar 10 已经被 pytorch 内置了，使用非常方便，只需要调用 `torchvision.datasets.CIFAR10` 就可以了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGNet\n",
    "vggNet 是第一个真正意义上的深层网络结构，其是 ImageNet2014年的冠军，得益于 python 的函数和循环，我们能够非常方便地构建重复结构的深层网络。https://blog.csdn.net/One2332x/article/details/122171029\n",
    "\n",
    "vgg 的网络结构非常简单，就是不断地堆叠卷积层和池化层，下面是一个简单的图示\n",
    "\n",
    "![](https://ws4.sinaimg.cn/large/006tNc79ly1fmpk5smtidj307n0dx3yv.jpg)\n",
    "\n",
    "vgg 几乎全部使用 3 x 3 的卷积核以及 2 x 2 的池化层，使用小的卷积核进行多层的堆叠和一个大的卷积核的感受野是相同的，同时小的卷积核还能减少参数，同时可以有更深的结构。\n",
    "\n",
    "vgg 的一个关键就是使用很多层 3 x 3 的卷积然后再使用一个最大池化层，这个模块被使用了很多次，下面我们照着这个结构来写一写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms as tfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以定义一个 vgg 的 block，传入三个参数，第一个是模型层数，第二个是输入的通道数，第三个是输出的通道数，第一层卷积接受的输入通道就是图片输入的通道数，然后输出最后的输出通道数，后面的卷积接受的通道数就是最后的输出通道数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(卷积层数,输入的通道数,输出的通道数)\n",
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    net = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), nn.ReLU(True)] # 定义第一层\n",
    "    \n",
    "    for i in range(num_convs-1): # 定义后面的很多层\n",
    "        net.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        net.append(nn.ReLU(True))\n",
    "        \n",
    "    net.append(nn.MaxPool2d(2, 2)) # 定义池化层\n",
    "    return nn.Sequential(*net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以将模型打印出来看看结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (5): ReLU(inplace=True)\n",
      "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "block_demo = vgg_block(3, 64, 128)\n",
    "print(block_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 150, 150])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "# 首先定义输入为 (1, 64, 300, 300)\n",
    "input_demo = Variable(torch.zeros(1, 64, 300, 300))\n",
    "output_demo = block_demo(input_demo)\n",
    "print(output_demo.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到输出就变为了 (1, 128, 150, 150)，可以看到经过了这一个 vgg block，输入大小被减半，通道数变成了 128\n",
    "\n",
    "下面我们定义一个函数对这个 vgg block 进行堆叠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_stack(num_convs, channels):\n",
    "    net = []\n",
    "    for n, c in zip(num_convs, channels):\n",
    "        in_c = c[0]\n",
    "        out_c = c[1]\n",
    "        net.append(vgg_block(n, in_c, out_c))\n",
    "    return nn.Sequential(*net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作为实例，我们定义一个稍微简单一点的 vgg 结构，其中有 8 个卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (3): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (4): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#共有5个vgg_block，每个vgg_block的卷积层数依次为(1, 1, 2, 2, 2)\n",
    "#每个vgg_block对应的（输入的通道数,输出的通道数）依次为\n",
    "#((3, 64), (64, 128), (128, 256), (256, 512), (512, 512))\n",
    "vgg_net = vgg_stack((1, 1, 2, 2, 3), ((3, 64), (64, 128), (128, 256), (256, 512), (512, 512)))\n",
    "print(vgg_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到网络结构中有个 5 个 最大池化，说明图片的大小会减少 5 倍，我们可以验证一下，输入一张 256 x 256 的图片看看结果是什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "test_x = Variable(torch.zeros(1, 3, 256, 256))\n",
    "test_y = vgg_net(test_x)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到图片减小了 $2^5$ 倍，最后再加上几层全连接，就能够得到我们想要的分类输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用批量标准化\n",
    "class vgg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(vgg, self).__init__()\n",
    "        self.feature = nn.Sequential(nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "                                     nn.BatchNorm2d(64),#(输出通道数)\n",
    "                                     nn.ReLU(True),\n",
    "                                     nn.MaxPool2d(2, 2),\n",
    "                                     \n",
    "                                     nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "                                     nn.BatchNorm2d(128),#(输出通道数)\n",
    "                                     nn.ReLU(True),\n",
    "                                     nn.MaxPool2d(2, 2),\n",
    "                                     \n",
    "                                     nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "                                     nn.BatchNorm2d(256),#(输出通道数)\n",
    "                                     nn.ReLU(True),\n",
    "                            \n",
    "                                     nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "                                     nn.BatchNorm2d(256),#(输出通道数)\n",
    "                                     nn.ReLU(True),\n",
    "                                     nn.MaxPool2d(2, 2),\n",
    "                                     \n",
    "                                     nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "                                     nn.BatchNorm2d(512),#(输出通道数)\n",
    "                                     nn.ReLU(True),\n",
    "                                     \n",
    "                                     nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "                                     nn.BatchNorm2d(512),#(输出通道数)\n",
    "                                     nn.ReLU(True),\n",
    "                                     nn.MaxPool2d(2, 2),\n",
    "                                     \n",
    "                                     nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "                                     nn.BatchNorm2d(512),#(输出通道数)\n",
    "                                     nn.ReLU(True),\n",
    "                                    \n",
    "                                     \n",
    "                                     nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "                                     nn.BatchNorm2d(512),#(输出通道数)\n",
    "                                     nn.ReLU(True),\n",
    "                                     \n",
    "                                     \n",
    "                                     nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "                                     nn.BatchNorm2d(512),#(输出通道数)\n",
    "                                     nn.ReLU(True),\n",
    "                                     nn.MaxPool2d(2, 2),\n",
    "                                     \n",
    "                                     \n",
    "                                     \n",
    "                                     \n",
    "                                     \n",
    "                                     \n",
    "                                    )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.2),# Dropout 放在需要drop神经元的层之前，传入每个神经元被舍弃的概率\n",
    "            nn.Linear(512, 100),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(100, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)#torch.Size([64, 3, 32, 32])\n",
    "        x = self.feature(x)\n",
    "        #print(x.shape)#torch.Size([64, 512, 1, 1])\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        #print(x.shape)#torch.Size([64, 512])\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net0 = vgg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "              ReLU-3           [-1, 64, 32, 32]               0\n",
      "         MaxPool2d-4           [-1, 64, 16, 16]               0\n",
      "            Conv2d-5          [-1, 128, 16, 16]          73,856\n",
      "       BatchNorm2d-6          [-1, 128, 16, 16]             256\n",
      "              ReLU-7          [-1, 128, 16, 16]               0\n",
      "         MaxPool2d-8            [-1, 128, 8, 8]               0\n",
      "            Conv2d-9            [-1, 256, 8, 8]         295,168\n",
      "      BatchNorm2d-10            [-1, 256, 8, 8]             512\n",
      "             ReLU-11            [-1, 256, 8, 8]               0\n",
      "           Conv2d-12            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-13            [-1, 256, 8, 8]             512\n",
      "             ReLU-14            [-1, 256, 8, 8]               0\n",
      "        MaxPool2d-15            [-1, 256, 4, 4]               0\n",
      "           Conv2d-16            [-1, 512, 4, 4]       1,180,160\n",
      "      BatchNorm2d-17            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-18            [-1, 512, 4, 4]               0\n",
      "           Conv2d-19            [-1, 512, 4, 4]       2,359,808\n",
      "      BatchNorm2d-20            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-21            [-1, 512, 4, 4]               0\n",
      "        MaxPool2d-22            [-1, 512, 2, 2]               0\n",
      "           Conv2d-23            [-1, 512, 2, 2]       2,359,808\n",
      "      BatchNorm2d-24            [-1, 512, 2, 2]           1,024\n",
      "             ReLU-25            [-1, 512, 2, 2]               0\n",
      "           Conv2d-26            [-1, 512, 2, 2]       2,359,808\n",
      "      BatchNorm2d-27            [-1, 512, 2, 2]           1,024\n",
      "             ReLU-28            [-1, 512, 2, 2]               0\n",
      "           Conv2d-29            [-1, 512, 2, 2]       2,359,808\n",
      "      BatchNorm2d-30            [-1, 512, 2, 2]           1,024\n",
      "             ReLU-31            [-1, 512, 2, 2]               0\n",
      "        MaxPool2d-32            [-1, 512, 1, 1]               0\n",
      "          Dropout-33                  [-1, 512]               0\n",
      "           Linear-34                  [-1, 100]          51,300\n",
      "             ReLU-35                  [-1, 100]               0\n",
      "           Linear-36                   [-1, 10]           1,010\n",
      "================================================================\n",
      "Total params: 11,639,126\n",
      "Trainable params: 11,639,126\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.76\n",
      "Params size (MB): 44.40\n",
      "Estimated Total Size (MB): 48.17\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "      net0 = net0.cuda()\n",
    "\n",
    "summary(net0, (3, 32, 32)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们可以训练我们的模型看看在 cifar10 上的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import train\n",
    "\n",
    "\n",
    "#使用数据增强\n",
    "def train_tf(x):\n",
    "    im_aug = tfs.Compose([\n",
    "        #tfs.Resize(32),\n",
    "        tfs.RandomHorizontalFlip(),\n",
    "        tfs.RandomRotation(45),\n",
    "        tfs.RandomVerticalFlip(),\n",
    "        #tfs.RandomCrop(20),\n",
    "        tfs.ColorJitter(brightness=0.5, contrast=0.5, hue=0.5),\n",
    "        tfs.ToTensor(),\n",
    "        #Normalize对每个通道执行以下操作：image =（图像-平均值）/ std在您的情况下，参数mean，std分别以0.5和0.5的形式传递。\n",
    "        #这将使图像在[-1,1]范围内归一化。\n",
    "        tfs.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    x = im_aug(x)\n",
    "    return x\n",
    "\n",
    "def test_tf(x):\n",
    "    im_aug = tfs.Compose([\n",
    "        tfs.ToTensor(),\n",
    "        #Normalize对每个通道执行以下操作：image =（图像-平均值）/ std在您的情况下，参数mean，std分别以0.5和0.5的形式传递。\n",
    "        #这将使图像在[-1,1]范围内归一化。\n",
    "        tfs.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        \n",
    "    ])\n",
    "    x = im_aug(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "train_set = CIFAR10('./data', train=True, transform=train_tf)\n",
    "train_data = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_set = CIFAR10('./data', train=False, transform=test_tf)\n",
    "test_data = torch.utils.data.DataLoader(test_set, batch_size=128,shuffle=False)\n",
    "\n",
    "net = vgg()\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=1e-1)\n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-2, momentum=0.9,weight_decay=1e-4) # 添加权重衰减系数\n",
    "#if torch.cuda.is_available():\n",
    "      #net = net.cuda()\n",
    "#optimizer = torch.optim.Adagrad(net.parameters(), lr=1e-2)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tensorboard设置，将路径设置为../outpue\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer0 = SummaryWriter(log_dir = '../output')#将条目直接写入output文件夹中的事件文件以供 TensorBoard 使用\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学习率修改函数\n",
    "def set_learning_rate(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(net, train_data, valid_data, num_epochs, optimizer, criterion):\n",
    "       \n",
    "    if torch.cuda.is_available():\n",
    "        net = net.cuda()\n",
    "    prev_time = datetime.now()\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch == 40:\n",
    "            set_learning_rate(optimizer, 0.005) # 第40 个epoch修改学习率为 0.005\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        net = net.train()\n",
    "        for im, label in train_data:\n",
    "            if torch.cuda.is_available():\n",
    "                im = Variable(im.cuda())  # (bs, 3, h, w)\n",
    "                label = Variable(label.cuda())  # (bs, h, w)\n",
    "            else:\n",
    "                im = Variable(im)\n",
    "                label = Variable(label)\n",
    "            # forward\n",
    "            output = net(im)\n",
    "            loss = criterion(output, label)\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.data.item()\n",
    "            train_acc += get_acc(output, label)\n",
    "                     \n",
    "        cur_time = datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        if valid_data is not None:\n",
    "            valid_loss = 0\n",
    "            valid_acc = 0\n",
    "            net = net.eval()\n",
    "            for im, label in valid_data:\n",
    "                if torch.cuda.is_available():\n",
    "                    im = Variable(im.cuda(), volatile=True)\n",
    "                    label = Variable(label.cuda(), volatile=True)\n",
    "                else:\n",
    "                    im = Variable(im, volatile=True)\n",
    "                    label = Variable(label, volatile=True)\n",
    "                output = net(im)\n",
    "                loss = criterion(output, label)\n",
    "                valid_loss += loss.data.item()\n",
    "                valid_acc += get_acc(output, label)\n",
    "            epoch_str = (\n",
    "                \"Epoch %d. Train Loss: %f, Train Acc: %f, Valid Loss: %f, Valid Acc: %f, \"\n",
    "                % (epoch, train_loss / len(train_data),\n",
    "                   train_acc / len(train_data), valid_loss / len(valid_data),\n",
    "                   valid_acc / len(valid_data)))\n",
    "        else:\n",
    "            epoch_str = (\"Epoch %d. Train Loss: %f, Train Acc: %f, \" %\n",
    "                         (epoch, train_loss / len(train_data),\n",
    "                          train_acc / len(train_data)))\n",
    "        prev_time = cur_time\n",
    "        print(epoch_str + time_str)\n",
    "        #增加将数据写入Writer\n",
    "        \n",
    "        writer0.add_scalar('Train Loss',  train_loss / len(train_data),  epoch)#添加标量数据Train Loss到事件文件writer0中\n",
    "        writer0.add_scalar('Train Acc',   train_acc / len(train_data),  epoch)\n",
    "        writer0.add_scalar('Valid Loss',  valid_loss / len(valid_data),  epoch)\n",
    "        writer0.add_scalar('Valid Acc',   valid_acc / len(valid_data),  epoch)\n",
    "    writer0.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1130/3668878487.py:41: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  im = Variable(im.cuda(), volatile=True)\n",
      "/tmp/ipykernel_1130/3668878487.py:42: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  label = Variable(label.cuda(), volatile=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train Loss: 2.009950, Train Acc: 0.250020, Valid Loss: 1.962028, Valid Acc: 0.263054, Time 00:01:07\n",
      "Epoch 1. Train Loss: 1.703365, Train Acc: 0.375360, Valid Loss: 1.534034, Valid Acc: 0.433643, Time 00:01:10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#from utils import train\n",
    "from datetime import datetime\n",
    "\n",
    "def get_acc(output, label):\n",
    "    total = output.shape[0]\n",
    "    _, pred_label = output.max(1)\n",
    "    num_correct = (pred_label == label).sum().data.item()\n",
    "    return num_correct / total\n",
    "\n",
    "train(net, train_data, test_data, 50, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，跑完 20 次，vgg 能在 cifar 10 上取得 76% 左右的测试准确率"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
